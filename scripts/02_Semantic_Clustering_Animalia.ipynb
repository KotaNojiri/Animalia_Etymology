{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wTRyGXzW0xp"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyFrhpRRVzm_"
   },
   "outputs": [],
   "source": [
    "# Load a Parquet file\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the Parquet file\n",
    "path = \"/User/path\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCn-pZmJW3um"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RojcKhq3V018"
   },
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load the model (prefer GPU if available)\n",
    "# ------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load the Species column from df\n",
    "# ------------------------------\n",
    "# df = pd.read_parquet(\"path_to_parquet\")  # ← your DataFrame\n",
    "\n",
    "# Replace NaN with empty strings (to avoid errors)\n",
    "texts = df[\"Species\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Generate embeddings\n",
    "# ------------------------------\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=128,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True   # Optional: normalize embeddings\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Add embeddings to df\n",
    "# ------------------------------\n",
    "df[\"Species_emb\"] = embeddings.tolist()\n",
    "\n",
    "# Sanity check\n",
    "print(df[\"Species_emb\"].head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0921pV0mV049"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "group_cols = [\n",
    "    \"LLM_Scientific_Abstract_Morphology\",\n",
    "    \"LLM_Scientific_Specific_Morphology\",\n",
    "    \"LLM_Scientific_Conceptual_Morphology\",\n",
    "    \"LLM_Scientific_People_Male\",\n",
    "    \"LLM_Scientific_People_Female\",\n",
    "    \"LLM_Scientific_Geography\",\n",
    "    \"LLM_Scientific_Other\",\n",
    "]\n",
    "\n",
    "# ---- This is likely already executed, but run again just in case ----\n",
    "df[group_cols] = df[group_cols].fillna(0).astype(int)\n",
    "row_sum = df[group_cols].sum(axis=1)\n",
    "\n",
    "# Use the leftmost 1 as the base Group\n",
    "df[\"Group\"] = df[group_cols].idxmax(axis=1)\n",
    "\n",
    "# If all zeros (None), assign to Other\n",
    "df.loc[row_sum == 0, \"Group\"] = \"LLM_Scientific_Other\"\n",
    "\n",
    "# Mask for multi-label rows\n",
    "multi_mask = row_sum > 1\n",
    "\n",
    "# Temporarily set multi-label rows to NaN (handled separately later)\n",
    "df.loc[multi_mask, \"Group\"] = np.nan\n",
    "\n",
    "# Merge People_Male / People_Female into People (single-label only)\n",
    "people_cols = [\n",
    "    \"LLM_Scientific_People_Male\",\n",
    "    \"LLM_Scientific_People_Female\",\n",
    "]\n",
    "df.loc[df[\"Group\"].isin(people_cols), \"Group\"] = \"LLM_Scientific_People\"\n",
    "\n",
    "print(\"Base Group counts (before 9-category mapping):\")\n",
    "print(df[\"Group\"].value_counts(dropna=False))\n",
    "\n",
    "# =====================================================\n",
    "# 1. Map single-label rows to categories 1–6\n",
    "# =====================================================\n",
    "\n",
    "# Create the final label column (9 categories)\n",
    "df[\"Group9\"] = np.nan\n",
    "\n",
    "# Only apply to single-label rows (i.e., not multi-label)\n",
    "single_mask = ~multi_mask\n",
    "\n",
    "single_map = {\n",
    "    \"LLM_Scientific_Specific_Morphology\":      \"1.LLM_Scientific_Specific_Morphology\",\n",
    "    \"LLM_Scientific_People\":                   \"2.LLM_Scientific_People\",\n",
    "    \"LLM_Scientific_Abstract_Morphology\":      \"3.LLM_Scientific_Abstract_Morphology\",\n",
    "    \"LLM_Scientific_Geography\":                \"4.LLM_Scientific_Geography\",\n",
    "    \"LLM_Scientific_Other\":                    \"5.LLM_Scientific_Other\",\n",
    "    \"LLM_Scientific_Conceptual_Morphology\":    \"6.LLM_Scientific_Conceptual_Morphology\",\n",
    "}\n",
    "\n",
    "# Convert Group to 1–6 labels for single-label rows\n",
    "df.loc[single_mask, \"Group9\"] = df.loc[single_mask, \"Group\"].map(single_map)\n",
    "\n",
    "# If single-label but Group9 is NaN (should be rare), assign to 5.Other\n",
    "df[\"Group9\"] = df[\"Group9\"].fillna(\"5.LLM_Scientific_Other\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Assign multi-label rows into categories 7/8/9\n",
    "# =====================================================\n",
    "\n",
    "# Extract only multi-label rows (0/1 columns)\n",
    "df_multi = df.loc[multi_mask, group_cols].copy()\n",
    "\n",
    "def multi_pattern_to_group9(row):\n",
    "    cols = [c for c in group_cols if row[c] == 1]\n",
    "    cols = sorted(cols)\n",
    "    pat = \" + \".join(cols)\n",
    "\n",
    "    if pat == \"LLM_Scientific_Abstract_Morphology + LLM_Scientific_Specific_Morphology\":\n",
    "        return \"7.LLM_Scientific_Abstract_Morphology + LLM_Scientific_Specific_Morphology\"\n",
    "    elif pat == \"LLM_Scientific_Abstract_Morphology + LLM_Scientific_Conceptual_Morphology\":\n",
    "        return \"8.LLM_Scientific_Abstract_Morphology + LLM_Scientific_Conceptual_Morphology\"\n",
    "    else:\n",
    "        return \"9.Other_Multi\"\n",
    "\n",
    "# Assign 7/8/9 labels to each multi-label row\n",
    "group9_multi = df_multi.apply(multi_pattern_to_group9, axis=1)\n",
    "\n",
    "# Write back to the original df\n",
    "df.loc[multi_mask, \"Group9\"] = group9_multi\n",
    "\n",
    "# =====================================================\n",
    "# 3. Verify results\n",
    "# =====================================================\n",
    "print(\"\\nGrouped into 9 categories (Group9):\")\n",
    "print(df[\"Group9\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZFNP_f9W-XC"
   },
   "source": [
    "## Visualization with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMY7xV4iV07i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# ======================================\n",
    "# 0. Parameters & output path\n",
    "# ======================================\n",
    "path = \"/workspace/d9999993/生物学の研究/語源解析/結果\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# ======================================\n",
    "# 1. Sampling & PCA (3D)\n",
    "#    (If df_sample is already loaded from a parquet file,\n",
    "#     this block can be skipped and df_sample can be loaded directly.)\n",
    "# ======================================\n",
    "\n",
    "# Group9 labels\n",
    "labels_full = df[\"Group9\"].astype(str).values\n",
    "unique_groups = pd.Series(labels_full).unique()\n",
    "\n",
    "print(\"Groups:\", unique_groups)\n",
    "\n",
    "# Sample up to 5000 points per Group9\n",
    "max_per_group = 5000\n",
    "rng = np.random.default_rng(42)\n",
    "sample_idx = []\n",
    "\n",
    "for g in unique_groups:\n",
    "    idx = np.where(labels_full == g)[0]\n",
    "    if len(idx) > max_per_group:\n",
    "        idx = rng.choice(idx, max_per_group, replace=False)\n",
    "    sample_idx.append(idx)\n",
    "\n",
    "sample_idx = np.concatenate(sample_idx)\n",
    "print(\"Total sampled points:\", len(sample_idx))\n",
    "\n",
    "# Create sampled DataFrame\n",
    "df_sample = df.iloc[sample_idx].copy()\n",
    "\n",
    "# Embedding subset\n",
    "X = np.vstack(df_sample[\"Species_emb\"].values)\n",
    "print(\"Embedding sample shape:\", X.shape)\n",
    "\n",
    "# PCA (3 dimensions)\n",
    "pca_3d = PCA(n_components=3, random_state=42)\n",
    "X_pca3 = pca_3d.fit_transform(X)\n",
    "\n",
    "df_sample[\"PCA3_x\"] = X_pca3[:, 0]\n",
    "df_sample[\"PCA3_y\"] = X_pca3[:, 1]\n",
    "df_sample[\"PCA3_z\"] = X_pca3[:, 2]\n",
    "\n",
    "# ======================================\n",
    "# 2. Multi-view (static) 3D visualization settings\n",
    "# ======================================\n",
    "\n",
    "groups = df_sample[\"Group9\"].unique()\n",
    "cmap = plt.get_cmap(\"tab20\", len(groups))\n",
    "\n",
    "# View candidates (elev: elevation angle, azim: azimuth angle)\n",
    "views_elev = [0, 15, 30, 45, 60, 75, 90]\n",
    "views_azim = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "views = [(e, a) for e in views_elev for a in views_azim]\n",
    "\n",
    "print(\"Number of views:\", len(views))   # 56\n",
    "print(\"First 10 views:\", views[:10])\n",
    "\n",
    "# ======================================\n",
    "# 3. Save one image per view\n",
    "# ======================================\n",
    "\n",
    "for i, (elev, azim) in enumerate(views, start=1):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    for j, g in enumerate(groups):\n",
    "        sub = df_sample[df_sample[\"Group9\"] == g]\n",
    "        ax.scatter(\n",
    "            sub[\"PCA3_x\"],\n",
    "            sub[\"PCA3_y\"],\n",
    "            sub[\"PCA3_z\"],\n",
    "            s=3,\n",
    "            alpha=0.6,\n",
    "            color=cmap(j),\n",
    "        )\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "    ax.set_title(f\"PCA3D e={elev}, a={azim}\", fontsize=10)\n",
    "\n",
    "    filename = f\"PCA3D_view_{i:02d}_e{elev}_a{azim}.png\"\n",
    "    save_path_view = os.path.join(path, filename)\n",
    "    plt.savefig(save_path_view, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved view {i:02d}: {save_path_view}\")\n",
    "\n",
    "# ======================================\n",
    "# 4. Save legend as a separate image\n",
    "# ======================================\n",
    "\n",
    "fig_legend = plt.figure(figsize=(10, 4))\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for j, g in enumerate(groups):\n",
    "    # Dummy scatter for legend (larger marker)\n",
    "    h = plt.scatter([], [], color=cmap(j), s=200)\n",
    "    handles.append(h)\n",
    "    labels.append(g)\n",
    "\n",
    "plt.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"center\",\n",
    "    fontsize=12,\n",
    "    ncol=3,   # 3 columns (adjust as needed)\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "legend_path = os.path.join(path, \"PCA3D_Group9_legend.png\")\n",
    "plt.savefig(legend_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig_legend)\n",
    "\n",
    "print(f\"Saved legend figure to: {legend_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/O67oTZ+CzAyznMW2nqKi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
